1.
Introduction:
The landscape of higher education has witnessed a growing emphasis on global rankings, with institutions trying to secure their positions on prestigious lists. One such influential ranking is the Financial Times (FT) master’s in management (MiM) ranking, which evaluates and ranks universities offering MiM programs based on various performance indicators. In today's world, where making decisions based on data is crucial, this project stands out as a smart move. Here, we are using data analysis and predictive modeling to help Odette School of Business to be in the FT ranking for master’s in management.
The primary objective of this project is to develop a robust predictive model that can forecast the potential ranking of universities in the ranking. By analyzing historical data spanning five years from 126 universities, we aim to identify key factors that significantly influence a university's ranking position. These insights will not only empower universities currently participating in FT rankings to enhance their strategies but will also serve as a valuable resource for universities considering participation in the future.
We have diligently assembled a rich dataset from multiple sources, including the Financial Times' historical MiM ranking data and additional external datasets with key information like faculty citations, social media metrics, research quality, and student-to-staff ratios, in order to achieve our goals. Using a variety of cutting-edge tools and methods for data analysis, we are concentrating on building a strong prediction model that is calibrated to determine the probability of universities moving up to the FT Times' esteemed top 30 rankings.
Universities within the top thirty benefit from heightened visibility and credibility, commanding attention from prospective students, industry leaders, and academic collaborators alike. The ranking not only reflects academic excellence but also serves as a testament to the institution's commitment to providing a high-caliber management education. For students exploring educational options, a position in the top thirty signals a commitment to quality and enhances the university's appeal as a destination for cutting-edge learning and research. This large-scale project demonstrates a dedication to improving academic institutions in a global education market that is becoming more and more competitive.
2.
Data Collection
In our capstone project, we delve into the dynamic landscape of higher education, merging traditional academic metrics with digital engagement indicators to assess the evolving profiles of modern universities. This blend of data, encompassing aspects like LinkedIn Followers, Instagram Engagement, Research Quality, and Citations per Faculty, is aimed at evaluating the dual influence of academic rigor and digital presence. Our hypothesis is that this combination shapes a university's reputation and success in the 21st century, offering a holistic view of their adaptability and standing in an ever-evolving educational environment.
2.1. Detailed Description of the Data Collection Process
The data collection process is a pivotal aspect of our research project. This phase was meticulously designed to gather comprehensive and relevant information that aligns with our project's objectives. We focused primarily on secondary data sources to accumulate a rich dataset pertaining to university rankings and various other parameters.
To ensure accuracy and reliability, our team utilized reputable sources. The key metrics concerning student-to-teacher ratio, student-to-staff ratio, and research quality were primarily extracted from globally
3
recognized educational databases, specifically the Times Higher Education1 and QS World University Rankings2 websites. These platforms are renowned for their comprehensive data on higher education institutions and were instrumental in providing us with credible and up-to-date information.
Additionally, we expanded our dataset to include modern metrics of university engagement and reach. To gather data on LinkedIn followers, we methodically navigated through the LinkedIn platform, focusing on the official university pages. This allowed us to understand the professional networking aspect of each university.
For insights into social media influence and engagement, particularly on Instagram, we utilized a combination of Instagram's analytics and Hyper auditor, a sophisticated digital tool designed to assess social media reach and engagement. This approach provided us with a nuanced understanding of each university's digital footprint and the effectiveness of their social media strategies.
2.2. Explanation of the variables included in the dataset:
The variables in the dataset can be grouped into two main categories based on their nature: Digital Engagement Metrics and Academic Performance Metrics. This categorization is aligned with the dual focus of our study — assessing the digital presence and academic prowess of universities.
Digital Engagement Metrics:
•
LinkedIn Followers: This measures the university's professional networking and outreach on LinkedIn, reflecting its connectivity in the professional world.
•
Instagram Engagement Percentage: Indicates the university's ability to engage and interact with audiences on a popular social media platform, demonstrating its digital marketing effectiveness and appeal to a younger demographic.
•
Google Rating: Reflects public perception and satisfaction rate, offering insight into the university's reputation among the public.
•
Google Number of Reviews: Provides a quantitative measure of public interaction and feedback, indicative of the university's visibility and engagement on a widely used platform.
Purpose of Collection: These metrics were collected to analyze the universities' digital footprint and engagement. In today's digitally driven world, a university's online presence and its ability to connect with its audience through social media and professional networks are critical indicators of its modern appeal and relevance.
Academic Performance Metrics:
•
Research Quality: Derived from academic databases, this assesses the university's contributions and standings in research, a traditional measure of academic excellence.
•
Citations per Faculty: Measures the impact and influence of the university’s research, as indicated by citations, highlighting the academic prestige and scholarly influence of the faculty.
•
Average Experience of Faculty: Provides insights into the faculty's expertise and professional background, indicating the level of experience and knowledge imparted to students.
1 World University Rankings 2024 | Times Higher Education (THE)
2 QS World University Rankings 2024: Top Global Universities | Top Universities
4
•
Students per Staff: A ratio that shows the university's commitment to resource allocation in terms of staffing, reflecting the learning environment and support available to students.
Purpose of Collection: These variables were chosen to gauge the academic rigor and quality of the universities. They offer a traditional perspective on the universities' core mission of education and research, allowing us to assess their academic standing and operational efficiency.
2.3.
Challenges Faced During Data Acquisition and Resolution Strategies
The data collection process, while thorough, was not without its challenges. One significant hurdle was ensuring the consistency and comparability of data from diverse sources. To address this, we established a rigorous data normalization process, which involved standardizing data formats and scales to maintain coherence throughout our dataset.
Another challenge was the dynamic nature of social media metrics, which can fluctuate frequently. To mitigate this, we chose specific time frames for data collection and employed tools like Hyper auditor to provide more stabilized and representative data points.
In conclusion, the careful selection of data sources and the strategic approach to overcoming challenges have ensured that our dataset is both comprehensive and reliable, providing a solid foundation for our subsequent analysis.
3.
Exploratory Data Analysis
3.1. Visualizations and summary statistics used for exploring data patterns.
Python was the key tool in our exploratory data analysis (EDA), revealing a plethora of statistical insights and visualisation trends within our dataset. The preliminary summary data provided a thorough picture of numerous variables such as LinkedIn followers, Instagram engagement percentage, salary increase, international board and faculty, research quality measures, faculty statistics, and so on. These statistics, which included counts, means, standard deviations, and percentiles, shed light on the dataset's primary tendencies, variations, and distributional characteristics across important parameters.
Scatterplots (figures 1-4, Appendix A) were used to reveal linkages between 'Rank' and other variables in order to acquire a better understanding of interrelationships. We were able to identify potential connections or dependencies between ranking variables and other qualities using this visualisation technique, revealing insight on aspects that might influence an institution's position or performance within the dataset.
Furthermore, histograms (figures 5-8, Appendix A) provided a visual narrative for understanding the distributional characteristics of particular elements. We created detailed histograms revealing the distribution patterns of several metrics using the Sweetviz tool. These graphical representations aided in the identification of skewness, central trends, and probable outliers across a wide range of features, allowing for a more sophisticated investigation of feature distributions.
3.2. Identification of trends, correlations, and key insights.
A heatmap (figures 9-10, Appendix A) proved to be an invaluable tool in analysing the patterns and correlations within our dataset, which had dimensions of 125 rows and 45 columns. This heatmap visualised relationships between numerous variables, highlighting prominent associations and presenting unexpected insights. Pairs such as 'International faculty (%)' in different years displayed an extraordinarily
5
strong correlation, demonstrating consistency or evolution over time within this metric. Similarly, metrics such as 'Average course length' and 'Weighted salary' showed strong correlations across years, indicating potential entangled influences or enduring patterns across these dimensions.
Correlations were discovered that highlighted essential characteristics of the dataset, revealing patterns that mirrored consistency, evolution, or reciprocal influences across years or within related measurements. These correlations and statistical information provided a more in-depth knowledge of the dataset's dynamics, shedding attention on variables that display synchronised patterns or significant influences on one another across time.
Furthermore, these connections act as leading signposts for further investigation, indicating prospective topics for in-depth research or feature significance in the context of the dataset's broader themes. By detecting these significant correlations, our study laid the groundwork for further investigation, providing later modelling and analytical efforts with a more detailed knowledge of the dataset's fundamental relationships and trends.
3.3.
Notable observations that influenced subsequent steps.
The detected correlations and patterns in our dataset provided some strong insights that influenced our subsequent steps and analytical orientation significantly:
Consistent International Faculty Trends: The good correlation between 'International faculty (%)' across different years indicates a strong stability or evolution within this metric over time. This observation prompted more concentrated research into the variables influencing this consistency, which led to deeper investigations into recruitment methods, institutional policies, and worldwide alliances affecting international faculty numbers.
Associations for Salary and Course Length: The strong correlation between 'Weighted salary' and 'Average course length' across many years suggested possible links between remuneration and programme lengths. This observation spurred a more in-depth investigation into the impact of compensation structures on programme lengths, as well as how this may influence student choices or institutional policies.
Faculty Composition Stability: The consistent correlations in 'Faculty with doctorates (%)' across years highlighted a consistent composition trend. This discovery prompted a thorough investigation into the impact of faculty composition on academic reputation, as well as institutional measures for attracting and keeping highly qualified faculty members.
These revelations influenced later phases by directing our attention to deeper study into these specific areas. They aided in targeted analyses, model feature selections, and the identification of locations for prospective interventions or optimisations within the educational environment.
4.
Data Preprocessing and Cleaning
4.1. Overview of data cleaning procedures.
Addressing missing values required a rigorous procedure that began with exploratory research using Excel functions to determine the degree of missing data across variables. Following that, Python's adaptability was used to apply custom rules and methods for imputation, filling in the missing values using various tactics such as mean substitution or predictive modelling where applicable. Concurrently, detecting and removing duplicate records was critical to ensuring the integrity of our dataset. We detected and deleted
6
unnecessary entries using a combination of Excel's deduplication functions and Python scripts, simplifying the dataset for later analysis.
Furthermore, our data preprocessing pipeline included strong procedures for dealing with outliers and standardising feature scales, which was critical for optimising the performance of our models. Outliers were detected and addressed prudently using a combination of statistical approaches in Excel and Python tools such as Pandas and NumPy, either through trimming, transformation, or capping methods. This step was critical in improving the robustness of our analysis by reducing the influence of extreme results. Furthermore, feature scaling was used to bring all variables within a standardised range, ensuring that no single feature unduly influenced the model due to its magnitude, hence improving the model's convergence and performance.
4.2. Handling of missing values and outliers.
Columns with more than 50 missing values in our 126-row dataset were regarded potentially biased if imputed. As a result, these columns were routinely removed in order to preserve the integrity of following studies. A pragmatic approach was taken for the remaining missing data in other columns. Imputation methodologies varied depending on the nature of the columns: mean or mode imputation was used when appropriate, ensuring that the process was adapted to the individual needs of each column. This strategy enabled more sophisticated treatment of missing data, avoiding the introduction of unnecessary bias while maintaining data integrity.
Addressing outliers required a thorough effort to ensure the dataset's dependability. Outliers were detected and deleted when values exceeded four standard deviations from the mean. Using statistical criteria, this robust strategy efficiently removed high data that could possibly influence subsequent studies or modelling attempts. We reduced the outliers' disproportionate influence on statistical analyses and modelling findings by methodically deleting them, resulting in a more accurate depiction of the underlying data trends and patterns.
4.3.
Standardization or normalization techniques applied.
Feature scaling was a critical stage in our research to guarantee that all variables contributed equally and significantly to the future analyses and modelling. We standardised our features by leveraging Python's libraries, specifically built-in functions like StandardScaler from Scikit-learn, to offset the impact of differing scales and units across distinct properties. The standardisation method was used to convert the data so that each feature had a mean of zero and a standard deviation of one. The data was effectively centred around zero with a unit variance, avoiding particular traits from dominating the study due to their original scale.
Our strategy was critical in preventing data leaks and preserving the integrity of our analyses. To that purpose, we strictly adhered to a tight protocol: before performing any imputation or scaling, we carefully divided the dataset into training and testing subsets. This separation allowed us to protect the testing data, ensuring that no information from the test set altered the scaling or imputation process inadvertently. All imputation and scaling techniques were applied solely to training data, preventing any knowledge of the test set from impacting the preprocessing steps.
We avoided data leaking and upheld the key idea of fair judgement on unseen data by following this careful procedure. The training set's standardised characteristics were then utilised to uniformly modify both the training and testing datasets, ensuring a coherent and fair assessment of model performance on previously unseen data. This methodical approach to feature scaling was critical in improving the robustness and generalizability of our models, which contributed to the dependability of our project's outputs.
7
5.
Feature Selection and Engineering
5.1. Techniques used for feature selection and their rationale.
Feature selection was a vital stage in improving our modelling technique in our project. To curate a subset of the most relevant features for our analysis, we used a multidimensional strategy including multiple methodologies. We began by doing an in-depth investigation of feature interrelationships, measuring correlations and utilising statistical tests to estimate their individual impact on our objective variables. We were able to discover redundancy or less informative features within strongly correlated sets thanks to this early investigation. Following that, we carefully verified and confirmed their importance in relation to our objectives by employing statistical tests and domain-specific hypotheses linked with each feature.
Several important aspects influenced our feature selection technique. Our primary goal was to improve model performance by deleting redundant or noisy attributes, lowering the risk of overfitting, and enhancing generalisation to new data. We attempted to prioritise variables that held significant influence or alignment with our hypotheses by scrutinising feature sets based on their statistical relevance and domain-specific hypotheses.
5.2.
Description of any new features created and their significance.
In order to improve the prediction model for university rankings, we engaged in feature engineering, creating novel attributes that had a significant impact on our predictive power and interpretative depth. These newly developed variables included both longitudinal viewpoints and stability indicators, expanding our dataset and having a substantial impact on the predictive modelling process.
The addition of characteristics generated from rank differentials between succeeding years, such as 'International course experience rank 2022-2021' and 'Value for money rank 2022-2021,' was critical. These characteristics served as dynamic indicators of institutional growth or regression across time, providing a window into the changing international landscape of academic institutions. Their inclusion gave historical depth to our predictive model, allowing for a full investigation of patterns driving rank changes.
Furthermore, stability-related characteristics such as 'Careers service Rank Stability' and 'Faculty Change' were critical in reflecting the consistency or fluctuations within institutional dynamics. These stability indicators captured the resilience or changes in specific dimensions over time, providing our model with insight into the long-term elements or volatile changes in university characteristics.
The importance of these new traits rests in their ability to capture complex temporal trends and patterns of stability within universities. These qualities improved the predictive model's capacity to identify the fundamental elements impacting university rankings by include the evolution over time and institutional stability. Their inclusion improved the model's performance greatly, allowing for more exact prediction of rank variations depending on the dynamic landscape of numerous institutional characteristics. Overall, these designed traits acted as critical pillars, increasing our model's predictive strength and explanatory ability in forecasting university rankings.
5.3.
Evaluation of the impact of feature engineering on model performance.
The study of feature engineering's impact on model performance was a critical exploration in our research centred on forecasting university rank probability. The addition of new features reflecting temporal trends and stability indicators, known as feature engineering, considerably affected the prediction model's efficacy in recognising rank changes across academic institutions.
8
The addition of new features, particularly those representing longitudinal perspectives such as 'International course experience rank 2022-2021' and 'Value for money rank 2022-2021,' resulted in significant improvements. These features contributed temporal dimension to our model, allowing it to represent changing institutional dynamics over time. By incorporating these temporal markers, the model developed a more comprehensive grasp of how institutional changes over time influenced rank probabilities, significantly improving its prediction capabilities.
Furthermore, stability-related characteristics such as 'Careers service Rank Stability' and 'Faculty Change' provided insights into the resilience or variations within key institutional aspects. Incorporating these stability criteria improved the model's capacity to understand long-term or rapid changes within universities, significantly improving its forecast accuracy.
According to our findings, the developed features considerably increased the model's performance. When models with and without these attributes were compared, there was a significant improvement in prediction accuracy, precision, and recall measures. The addition of these engineering traits improved the model's ability to detect tiny differences in university characteristics, resulting in a more accurate assessment of rank probability.
5.4.
Final set of selected features
The final step in feature engineering and assessment is selecting the set of most relevant and effective features to build the model on. Therefore, based on what explained above, we selected below features to use in our model building process. All the features and data are available in the final excel submitted with this report.
LinkedIn Followers, Instagram engagement %, Research Quality, Citations per Faculty, Average Experience of Faculty, Students per Staff, 2020 Faculty with doctorates (%), 2020 Weighted salary (US$), 2020 International faculty (%), 2020 International mobility rank, 2020 International board (%), 2020 Extra languages, 2020 Average course length (months), 2020 Overall satisfaction, 2020 International students (%), 2021 Women on board (%), 2021 Employed at three months (%), Delimited, 2021 International faculty (%), 2021 International mobility rank, 2021 Female students (%), 2022 Careers service rank, 2022 International course experience rank, 2022 Internships (%), 2022 Faculty with doctorates (%), Rank in 2021, 2022 Average course length (months), 2022 Overall satisfaction, 2022 Women on board (%), 2022 Female students (%), 2022 Female faculty (%), 2022 Career progress rank, 2022 Salary percentage increase, 2022 Weighted salary (US$), International course experience rank 2022-2021, International work mobility rank 2022-2021, Value for money rank 2022-2021, Careers service Rank Stability, International course experience Rank Stability, International work mobility Rank Stability, Career progress Rank Stability, Value for money Rank Stability, Faculty Change, Satisfaction Change, Value for Money Index.
6.
Predictive Model Building
The development of predictive models is a critical component of data-driven decision-making, providing insights into future events. In this endeavour, our effort focused on developing a robust prediction model to determine the likelihood of colleges reaching the prestigious top 30 ranks of the FT Times. Using advanced machine learning techniques and substantial data analysis, we set out to construct a binary classification system that would identify colleges that were likely to place in the top 30 (1) from those that would not (0). This project aimed to empower educated judgements that are critical for improving educational excellence and strategic developments in academia.
9
6.1.
Choice of modeling algorithms and justification.
In our quest to estimate university rank probability, we carefully evaluated and compared numerous modelling techniques to determine the best approach. CatBoost emerged as our top pick after extensive testing due to its robust handling of categorical features and efficient processing of huge datasets. CatBoost performed well on our binary classification job, with a ROC AUC score of 0.83 and an outstanding accuracy score of 0.92 on the test dataset. This algorithm provided a good balance of predictive strength and computational efficiency.
6.2.
Model training process and performance assessment
To reduce data leaks and ensure model generalisation, we used an 80-20 train-test data split. To prevent information leakages, the preprocessing pipeline used rigorous imputation and scaling, on only the training data. For each method, hyperparameter tweaking was critical, optimising parameters such as learning rates, tree depth, and iterations using approaches such as grid search and cross-validation. This procedure improved model performance by increasing predicted accuracy and ensuring robustness against overfitting.
We used critical assessment measures appropriate for our binary classification task to assess model performance. As our major indicator, we used ROC AUC (Receiver Operating Characteristic - Area Under Curve) to assess the model's ability to distinguish between positive and negative classes. In addition, accuracy scores were calculated to determine the proportion of accurately anticipated events. These measures offered a thorough insight of the model's prediction capacity and ability to distinguish different university rankings.
6.3.
Final selected model and Important Features
On the test dataset, our final CatBoost model performed quite well, with a ROC AUC score of 0.83 and an accuracy score of 0.92. It was suitable for our classification challenge due to its robust handling of categorical variables, quick training on huge datasets, and good predictive power. Despite competitive performance from Adaboost, GBboost, and RandomForest, CatBoost demonstrated a balance of model complexity, predictive accuracy, and computational efficiency, solidifying its position as our model of choice for predicting university rank probabilities.
The model emphasised numerous key features in forecasting university rank probabilities, emphasising their significance in the predictive process. '2020.Faculty with doctorates (%)' has the highest importance (11.70), indicating a strong influence on rank probability. '2020.Weighted salary (US$)' and '2022.Weighted salary (US$)' followed, demonstrating their significant impact on the predicted model (6.60 and 6.49, respectively). Furthermore, measures such as 'International mobility rank' over several years and 'LinkedIn Followers' were highly valued.
7.
Model Validation and Tuning
7.1.
Validation strategy and performance metrics
A strong validation technique was critical in our predictive modelling study, which aimed to forecast university rank probability. We used stratified 5-fold cross-validation because of its effectiveness in maintaining class distribution within folds. Given the dataset's disparity in rankings, our technique ensured that each fold represented a proportionate distribution of top-ranked and lower-ranked colleges. The
10
stratification kept the original class distribution across folds in both training and validation sets, reducing bias and allowing a more trustworthy assessment of model performance.
We painstakingly monitored model performance throughout the validation process using a battery of performance measures customised to our binary classification problem. ROC AUC (Receiver Operating Characteristic - Area Under Curve) and accuracy ratings were among the measurements used. The ROC AUC statistic accurately quantified the model's ability to differentiate between positive and negative classes across all thresholds, whereas accuracy offered an overall measure of correct predictions. We gained a comprehensive picture of the model's consistency and generalisation across distinct subsets of data by leveraging these metrics on each fold of the cross-validation.
7.2.
Adjustments made based on validation results.
The validation results were critical in fine-tuning and optimising our predictive model. We iteratively changed specific hyperparameters based on the validation results to improve model performance. Notably, improvements were made to CatBoost, our chosen model, in terms of learning rates, tree depth, and iterations. These changes were made to establish a compromise between model complexity and predicted accuracy, with the goal of avoiding overfitting while maximising performance on unknown data. In addition, we ran feature selection loops, deleting features that contributed little to model performance. The validation findings led the iterative changes, ensuring that our model was robust, well-generalized, and capable of accurately forecasting university rank probabilities. Finally, these validation-based tweaks considerably contributed to improving our model's predictive performance and stability across varied datasets.
8.
Recommendations and Conclusion
8.1.
Discussion of potential improvements or future work.
A strong basis for comprehending the variables impacting university rankings in the Financial Times Master's in Management (FT MiM) ranking has been established by the analysis. Nonetheless, there are directions for further research and areas for development that might raise the quality and scope of the insights:
•
Temporal Examination of Extrinsic Elements:
University-specific internal measures were the main focus of the analysis. A more thorough analysis of outside variables that can affect rankings, such as modifications to industry trends, changes in the state of the world economy, and adjustments to educational policy, may be the focus of future research. A more complete view might come from a temporal investigation of how these outside factors affect ranking dynamics over time.
•
Machine Learning Model Optimization:
Although CatBoost shown good performance, more research might investigate how to optimize machine learning models through the use of various algorithms, hyperparameters, and ensemble techniques. A more accurate and refined predictive model may result from this iterative process, which would help universities looking to raise their rankings make more informed decisions.
•
Longitudinal Analysis of Rank Stability:
11
During the feature engineering phase, stability indicators like "Faculty Change" and "Careers service Rank Stability" were included. These indicators provide important long-term trend insights. Subsequent research endeavors may entail an in-depth, longitudinal examination of these stability indicators, revealing trends in institutional resilience or pinpointing areas that require ongoing enhancements.
The analysis can develop into a more dynamic and comprehensive framework by addressing these potential improvements and pursuing further research in these areas. This will give universities increasingly sophisticated and practical strategies for success in the cutthroat field of management education rankings.
8.2.
Overall conclusion and project summary.
To sum up, this analytical effort is an extensive and calculated attempt to raise the Odette School of Business's profile in the Financial Times Master of Management ranking. The study discovered important factors influencing university rankings and produced actionable insights for focused changes using rigorous data collecting, exploratory data analysis, and predictive modeling. The creation of a reliable predictive model with the CatBoost algorithm was made possible by the application of cutting-edge statistical approaches like feature engineering and model validation. The elements that have been chosen, which include social media metrics, faculty traits, and international indicators, provide a useful starting point for universities that aim to maximize their competitive advantages in the management education market.
Furthermore, the project's emphasis on a strict pipeline for preparing data, which includes addressing outliers and missing values as well as standardizing feature scales, guarantees the accuracy and consistency of the results. The project's dedication to methodological rigor is highlighted by the thorough discussion of the difficulties encountered during data collecting and the strategic resolution techniques used. All things considered, this report is a useful tool for strategic planning and well-informed decision-making in the field of higher education. It not only offers a roadmap for Odette School of Business, but it also provides guidance for other universities hoping to successfully negotiate the complexities of global rankings.